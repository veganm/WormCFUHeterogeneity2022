---
title: "False Positive Assessment On Counts"
output: html_notebook
---

## Purpose
This notebook accompanies the code in the corresponding manuscript "Sample pooling inflates error rates in between-sample comparisons: an empirical investigation of the statistical properties of count-based data". The code contained here is intended to allow end users to interrogate their own count-based data using the approaches in that manuscript.

This code was written in R 4.4.0 running in Rstudio 2024.04.0 build 735.


## Housekeeping and Functions
Before we do anything, we will need to load the packages this code depends on. Here we load the required packages, using PACkageMANager to install any that are missing.
```{r}
##install.packages("pacman") ## Uncomment and run if pacman is not already installed
pacman::p_load(ggplot2, tidyverse, cowplot, mclust, e1071, ggpubr, ggpmisc, readxl, patchwork, sBIC, lmtest, car)
## and create a global variable
xTextSize<-14
```

Now we will create some functions that will be needed below. Make sure to run these code blocks before proceeding to the next section.  

The *bootOnCounts()* function provides the basic functionality to bootstrap on raw count data, which may or may not be from serially diluted samples:
```{r}
bootOnCounts<-function(n_reps, mydata, batch_sizes=c(1,5,10,20,50), FoldD=10, correction_constant=20){
  ## Expects a number of replicates for the bootstrap (n_reps)
  ## and a data frame where each row represents one individual (mydata)
  ## where the number of colonies counted is in column "Count"
  ## the dilution at which these colonies were measured is in column "D",
  ##   e.g. D=0 for undiluted sample, D=1 for the first FoldD-fold dilution, etc
  ## FoldD(num) is fold dilution in dilution series 
  ##  (Default is 10X dilutions, e.g. each step in serial dilution is 1:10 volume)
  ##
  #### The dilution correction factor (numeric) is given as "correction_constant"
  ## and is the ratio of the original volume and the volume plated/measured
  ## e.g. for 10 uL spots and an original volume of 1 mL, correction_constant = 100
  ## 
  ## Returns a data frame of simulated batch digests
  ## with batch sizes in vector batch_sizes, default (1, 5, 10, 20, 50) individuals/batch
  ## values reported as inferred CFU/individual and log10(CFU/individual)
  
  ## get size of data
  capp<-dim(mydata)[1]
  
  ## make someplace to put stuff
  temp<-vector("list", length=length(batch_sizes)*n_reps)
  
  for(i in seq_len(n_reps)){
    batch_data<-rep(0, length(batch_sizes))
    for (j in seq_along(batch_sizes)){
      ## Randomly pull samples from data for batching
      idx<-sample(1:capp,batch_sizes[j],replace=TRUE)
      ## Assume Poisson count error and generate new counts
      temp_count<-rpois(batch_sizes[j], mydata$Count[idx])
      ## Calculate CFU/worm
      batch_data[j]<-mean(correction_constant*temp_count*FoldD^mydata$D[idx], na.rm=TRUE)
    }
    temp[[i]]<-tibble(Batch=batch_sizes,
                      FinalCount=batch_data) 
  }
  
  ## unfold data
  dataSet<-dplyr::bind_rows(temp) ## unpack
  dataSet$FinalCount[dataSet$FinalCount<1]<-1
  dataSet<-dataSet %>%
    mutate(logCFU=log10(FinalCount))
  if (sum(is.infinite(dataSet$logCFU))>0){dataSet$logCFU[is.infinite(dataSet$logCFU)]<-0.1}
  dataSet$logCFU[dataSet$logCFU<0]<-0.1
  
  return(dataSet)
}
```

The *bootOnCountsStats()* function does the heavy lifting on the bootstraps that will be run in this script. Note that the function will technically accept batch-based data, but the stability of this code for non-uniformly weighted batches has not been tested.
```{r}
bootOnCountsStats<-function(input_data, batch_sizes=c(1,5,10,20,50), nboot=1000, 
                            FoldD=10, correction_constant=20){
  ## Function that performs many iterations of bootstrapping on counts
  ## Assumes all data sets (indicated by unique values of Run) are independent replicates of one experiment
  ## If this is true, results will indicate run-to-run variation and estimated false-positive rates
  ## Calls bootOnCounts() for basic functionality.
  ## 
  ## IF A DATA SET OF INDIVIDUAL-BASED DATA FOR ONE EXPERIMENTAL CONDITION IS PROVIDED
  ##   - The data set must contain at least two runs of data, ideally with 18+ data points in each
  ##   - Function assumes no subsampling (each individual is essentially a unit-1 subsample)
  ##   - Generates resampled data plus Poisson noise for individual and batch-based measurements
  ##   - Returns data statistics and t-test and Wilcoxon results of nboot resamples for each pair of samples at each batch size
  ##
  ## IF A DATA SET OF BATCH-BASED DATA FOR ONE EXPERIMENTAL CONDITION IS PROVIDED
  ##   - The data set must contain at least two runs of data, ideally with 18+ data points in each
  ##   - Generates resampled data plus Poisson noise using the indicated weighting from input data
  ##   - Returns data statistics and t-test and Wilcoxon results of nboot resamples for each pair of samples at the batch size in data
  ## 
  ## IF A DATA SET REPRESENTING MORE THAN ONE EXPERIMENTAL CONDITION IS PROVIDED
  ##   - The data set must be as already described
  ##   - Returns a list containing two objects
  ##      * data_summary: Data statistics and t-test and Wilcoxon results of nboot resamples for each pair of samples at each batch size
  ##      * regression_summary: Statistics of glm and ANOVAs for each pair of experimental conditions
  ##
  ## Takes a data set with columns
  ##   Run (num): Each unique index represents one data set
  ##   Count (num): Number of raw counts associated with each sample
  ##   Batch (int): batch size or weight of each sample.
  ##         Batch=1 is the minimum and indicates individual-based sampling.
  ##   D (num): Fold dilution at which counts were taken
  ##         (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc)
  ##   FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling.
  ##         Will be calculated if not present.
  ##   Condition (chr, optional): Experimental condition. 
  ##          Will not be used unless data set contains more than one unique value for Condition.
  ## Each row of the data set represents one measurement.
  ## 
  ## Other inputs:
  ##   batch_sizes (int): If suppling individual measurements, a vector of batch sizes is needed for
  ##   FoldD (num): Fold dilution in dilution series. Default is 10X dilutions.
  ##   correction_constant (num): a multiplier to correct the fraction of a sample used for one measurement to the original volume
  ##        (e.g. counts from 10 uL spots and an initial volume of 1 mL require correction (1000/10)=100)
  
  ## load the necessary
  pacman::p_load(tidyverse)
  
  #### START
  ## Figure out what is in the data set
  ## Do we need to correct counts for dilution?
  ## Divide by batch size (may be 1) to get per-unit or per-individual numbers
  my_names<-names(input_data) 
  if(length(which(my_names=="FinalCount"))==0){  ## if FinalCount does not exist
    if(length(which(my_names=="D"))!=0){ ## if we are given a dilution factor
      input_data$FinalCount<-(correction_constant *(input_data$Count*FoldD^input_data$D))/Batch
    } else if(length(which(my_names=="D"))==0){
      stop("Fold dilution D must be a column in input_data!")
    }
      else{
      input_data$FinalCount<-(correction_constant * input_data$Count)/Batch
    }
  } 
  ## now we should have final counts
  
  
  ## How many experimental conditions are in the data set?
  condition_list<-unique(input_data$Condition)
  n_conditions<-length(condition_list)
  
  ## How many runs of data are in the input data set?
  input_data$RunID<-paste(input_data$Condition, input_data$Run, sep="")
  run_ids<-unique(input_data$RunID)
  n_runs<-length(run_ids)
  if(n_runs<2){
    stop("At least two runs of data are required for bootstrapping.")
  }
  
  ## Are data individual or batch-based?
  isIndividual<-max(input_data$Batch) == 1

    # Correct batching if batch data are supplied
  if (!isIndividual){  #### For batch-based data
    batch_sizes<-max(input_data$Batch) # this should return a scalar - only one "batch size" in data
    }

  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  ## Someplace to put the data generated
  ###  Temporary list object for summary statistics of pairs
  temp<-vector("list", length=length(batch_sizes)*factorial(n_runs)*nboot)
  ### Temporary data storage for regressions and regression stats if data have replicates
  if(n_conditions>1){
    temp_data<-vector("list", length=n_runs) #Assemble bootstrapped replicates, including one of each unique replicate (x) condition
    temp_reg<-vector("list", length=nboot*length(batch_sizes)) # at the end of a boot, do comparisons on replicate-structured data
  }
  
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # Start bootstrap loop    
  
  idx<-1  ## initiate looping index
  idx_set<-1
  for (m in seq_len(nboot)){  
    idx_pair<-1  # index for combining data sets within boot
    for (i in seq_len(n_runs)){
        for (j in i:n_runs){ ## For each pair of runs
            ## Obtain two sets of data from the input
            temp1<-input_data %>%
              dplyr::filter(RunID==run_ids[i])
            temp2<-input_data %>%
              dplyr::filter(RunID==run_ids[j])
            
            ## Bootstrap both sets
            n1<-dim(temp1)[1]
            n2<-dim(temp2)[1]
            
            if(isIndividual){ # can use bootOnCounts() to bootstrap individual-based data
              Boot1<-bootOnCounts(n_reps=n1, mydata=temp1, 
                                  batch_sizes=batch_sizes, FoldD=FoldD, correction_constant = correction_constant)
              Boot2<-bootOnCounts(n_reps=n2, mydata=temp2, 
                                  batch_sizes=batch_sizes, FoldD=FoldD, correction_constant = correction_constant)
            } else{ # but batched data can't be extrapolated to other batch sizes
              ## Resamples with batch weights from data
              idx1<-sample(1:n1,n1,replace=TRUE)
              idx2<-sample(1:n2,n2,replace=TRUE)
              ## Assume Poisson count error and generate new counts
              temp_count_1<-rpois(n1, temp1$Count[idx1])
              temp_count_2<-rpois(n2, temp2$Count[idx2])
              ## Calculate biologically averaged total counts for resampled data
              Boot1counts<-sort((correction_constant*temp_count_1*FoldD^temp1$D[idx1])/temp1$Batch)
              Boot2counts<-sort((correction_constant*temp_count_2*FoldD^temp2$D[idx2])/temp2$Batch)
              # correct for any inadmissible values
              Boot1counts[Boot1counts<1]<-1
              Boot2counts[Boot2counts<1]<-1
              # assemble into objects with the same format as bootOnCounts() output
              Boot1<-tibble(Batch=batch_sizes,  # just so loop on batch_sizes works properly
                            FinalCount=Boot1counts,
                            logCFU=log10(Boot1counts)
                            )
              Boot2<-tibble(Batch=batch_sizes, # just so loop on batch_sizes works properly
                            FinalCount=Boot2counts,
                            logCFU=log10(Boot2counts)
                            )
            } # end conditional generation of bootstrapped data
                      
            ## fix any zeros thrown by resample
            if (sum(is.infinite(Boot1$logCFU))>0){Boot1$logCFU[is.infinite(Boot1$logCFU)]<-0.1}
            if (sum(is.infinite(Boot2$logCFU))>0){Boot2$logCFU[is.infinite(Boot2$logCFU)]<-0.1}
            Boot1$logCFU[Boot1$logCFU<=0]<-0.1
            Boot2$logCFU[Boot2$logCFU<=0]<-0.1
    
            ## generate stats and carry out tests for pairs of data sets
            for (k in seq_along(batch_sizes)){
              Boot1s<-Boot1 %>%
                filter(Batch==batch_sizes[k])
              Boot2s<-Boot2 %>%
                filter(Batch==batch_sizes[k])
              Boot.t<-t.test(Boot1s$logCFU, Boot2s$logCFU)
              Boot.w<-wilcox.test(Boot1s$logCFU, Boot2s$logCFU, exact=FALSE)
              sw1<-shapiro.test(Boot1s$logCFU)
              sw2<-shapiro.test(Boot2s$logCFU)
              ## sort data for quantile calculations
              Boot1s<-sort_by(Boot1s, Boot1s$logCFU)
              Boot2s<-sort_by(Boot2s, Boot2s$logCFU)
              
            ## get values for Hogg's calculations##
              L05_1=mean(Boot1s$FinalCount[1:ceiling(n1/20)])
              U05_1=mean(Boot1s$FinalCount[-(1:(n1-ceiling(n1/20)))], na.rm=TRUE)
              L5_1=mean(Boot1s$FinalCount[1:round(n1/2)], na.rm=TRUE)
              U5_1=mean(Boot1s$FinalCount[-(1:round(n1/2))], na.rm=TRUE)
              M5_1=mean(Boot1s$FinalCount[ceiling(n1/4):floor(n1*0.75)], na.rm=TRUE)
              L05_2=mean(Boot2s$FinalCount[1:ceiling(n2/20)])
              U05_2=mean(Boot2s$FinalCount[-(1:(n2-ceiling(n2/20)))], na.rm=TRUE)
              L5_2=mean(Boot2s$FinalCount[1:round(n2/2)], na.rm=TRUE)
              U5_2=mean(Boot2s$FinalCount[-(1:round(n2/2))], na.rm=TRUE)
              M5_2=mean(Boot2s$FinalCount[ceiling(n2/4):floor(n2*0.75)], na.rm=TRUE)
              
              ## store the results
              temp[[idx]]<-tibble(
                Boot=m,
                #Batch1=temp1$Batch,  # note: 
                #Batch2=temp2$Batch,  # Batch1 and Batch2 from data if data are batched
                Batch=batch_sizes[k], # Batch is from simulations if data are individual
                Condition1=temp1$Condition[1],
                Condition2=temp2$Condition[1],
                Run1=temp1$Run[1],
                Run2=temp2$Run[1],
                meanCFU1=mean(Boot1s$FinalCount, na.rm=TRUE), 
                meanCFU2=mean(Boot2s$FinalCount, na.rm=TRUE),
                varCFU1=var(Boot1s$FinalCount, na.rm=TRUE), 
                varCFU2=var(Boot2s$FinalCount, na.rm=TRUE),
                cvCFU1=sd(Boot1s$FinalCount, na.rm=TRUE)/mean(Boot1s$FinalCount, na.rm=TRUE),
                cvCFU2=sd(Boot2s$FinalCount, na.rm=TRUE)/mean(Boot2s$FinalCount, na.rm=TRUE),
                Q1.1=(U05_1-M5_1)/(M5_1-L05_1), ## Hogg's stats
                Q2.1=(U05_1-L05_1)/(U5_1-L5_1),
                Q1.2=(U05_2-M5_2)/(M5_2-L05_2),
                Q2.2=(U05_2-L05_2)/(U5_2-L5_2),
                p.sw1=sw1$p.value, 
                p.sw2=sw2$p.value,
                p.t=Boot.t$p.value,
                p.w=Boot.w$p.value
              )
              idx<-idx+1
              } ## finish loop over batch sizes (k)
            
            # If needed - Assemble data across run pairs
            if(n_conditions>1 & i==j){
              Boot1$Condition<-temp1$Condition[1]
              #Boot2$Condition<-temp2$Condition[1]
              Boot1$Run<-temp1$Run[1]
              #Boot2$Run<-temp2$Run[1]
              # combine data 
              #Boot12<-rbind(Boot1, Boot2)
              #Boot12$Boot<-m
              #temp_data[[idx_pair]]<-Boot12
              Boot1$Boot<-m
              temp_data[[idx_pair]]<-Boot1
              idx_pair<-idx_pair+1
            }
              
          } } ## finish loops over run pairs (i, j)
    
    # within each bootstrap, do comparisons with replicate-structured data
    if(n_conditions>1){
      # unpack data set
      temp_data_long<-dplyr::bind_rows(temp_data)
      temp_data_long$Run<-factor(temp_data_long$Run)
      temp_data_long$logCFU[temp_data_long$logCFU<=0]<-0.1
      
      # First have to parse out by batch size
      for (k in seq_along(batch_sizes)){
              temp_data_long_s<-temp_data_long %>%
                filter(Batch==batch_sizes[k])
              # Then pair experimental conditions
              for (q in 2:n_conditions){
                #print(c(k, q))  #debug
                temp_data_long_s1<-temp_data_long_s %>%
                  filter(Condition==condition_list[q-1] | Condition==condition_list[q])
                #print(unique(temp_data_long_s1$Condition)) # debug
                #print(min(temp_data_long_s1$FinalCount))
                #print(min(temp_data_long_s1$logCFU))
                # Inside batch size and for pairs of conditions, do tests
                Boot.t<-t.test(logCFU~Condition, data=temp_data_long_s1)
                Boot.w<-wilcox.test(logCFU~Condition, data=temp_data_long_s1, exact=FALSE)

              # glm - will retrieve significance for "Condition"
               myglm<-temp_data_long_s1 %>%
                 glm(logCFU~Condition+Run, family=Gamma, data=.)
               
               #print(summary(myglm))  # DEBUG
                
               myglm.nested<-temp_data_long_s1 %>%
                  glm(logCFU~Condition, family=Gamma, data=.)
               myglm.unique<-temp_data_long_s1 %>%
                glm(logCFU~Condition*Run, family=Gamma, data=.)
               
              myglm.lrt<-lrtest(myglm, myglm.nested)
              myglm.lrtu<-lrtest(myglm, myglm.unique)
              glm_dim<-dim(coef(summary(myglm)))
              #print(AIC(myglm)) # debug
              #print(AIC(myglm.unique)) # debug
              
              # ANOVA
              my.levene<-leveneTest(logCFU~Condition+Run, data=temp_data_long_s1)
              my.aov<-aov(logCFU~Condition*Run, data=temp_data_long_s1)
            
              #print(summary(my.aov))  #debug
            
              }
              # Store test results
              temp_reg[[idx_set]]<-tibble(
                        Run=m,
                        batch=batch_sizes[k],
                        Set1=condition_list[q-1],
                        Set2=condition_list[q],
                        p.t=Boot.t$p.value,
                        p.w=Boot.w$p.value,
                        p.glm=coef(summary(myglm))[glm_dim[1],glm_dim[2]], # obtain p value for Condition2
                        p.glm.lrt=myglm.lrt$`Pr(>Chisq)`[2],
                        p.glm.lrtu=myglm.lrtu$`Pr(>Chisq)`[2],
                        p.glm.AIC=exp((AIC(myglm)-AIC(myglm.nested))/2),
                        p.glm.AICu=exp((AIC(myglm.unique)-AIC(myglm))/2),
                        p.aov=summary(my.aov)[[1]][2,5],
                        p.levene=my.levene$`Pr(>F)`[1]
              )
              idx_set<-idx_set+1 # advance index for replicate-based test results
      } # end internal loop over batch sizes
    } # end replicate-structured comparisons
    
    } ## finish loop over bootstraps (m)
#  } 
#else {## finish individual conditional
#    ########
#    ##    IF ONLY BATCHED DATA ARE AVAILABLE
#    ## Someplace to put the data generated
#    temp<-vector("list", length=n_runs*n_runs*nboot)
#    
#    ## If we have batched data
#    ## Can't extrapolate to other batch sizes, but we can generate summary stats for the batching in data
#    for (m in seq_len(nboot)){  
#      ##print(m) ##debug
#      for (i in seq_len(n_runs)){
#        for (j in seq_len(n_runs)){ ## For each pair of runs
#          ##print(c(i,j)) ## debug
#          ## Obtain two sets of data from the input
#          temp1<-input_data %>%
#            dplyr::filter(Run==run_ids[i])
#          temp2<-input_data %>%
#            dplyr::filter(Run==run_ids[j])
#          n1<-dim(temp1)[1] ## get sizes
#          n2<-dim(temp2)[1]
          ### Resamples with batch weights from data
#          idx1<-sample(1:n1,n1,replace=TRUE)
#          idx2<-sample(1:n2,n2,replace=TRUE)
#          ## Assume Poisson count error and generate new counts
#          temp_count_1<-rpois(n1, temp1$Count[idx1])
#          temp_count_2<-rpois(n2, temp2$Count[idx2])
#          ## Calculate biologically averaged total counts for resampled data
#          Boot1s<-(correction_constant*temp_count_1*FoldD^temp1$D[idx1])/temp1$Batch
#          Boot2s<-(correction_constant*temp_count_2*FoldD^temp2$D[idx2])/temp2$Batch
#          
#          Boot1s_log<-log10(Boot1s)
#          Boot1s_log[!is.finite(Boot1s_log)]<-0
#          Boot2s_log<-log10(Boot2s)
#          Boot2s_log[!is.finite(Boot2s_log)]<-0
          
#          Boot.t<-t.test(Boot1s_log, Boot2s_log)
#          Boot.w<-wilcox.test(Boot1s, Boot2s, exact=FALSE)
#          sw1<-shapiro.test(Boot1s_log)
#          sw2<-shapiro.test(Boot2s_log)
          
          ## sort data for quantile calculations
#          Boot1s<-sort(Boot1s)
#          Boot2s<-sort(Boot2s)
          
          ## get values for Hogg's calculations##
 #           L05_1=mean(Boot1s[1:ceiling(n1/20)])
#            U05_1=mean(Boot1s[-(1:(n1-ceiling(n1/20)))], na.rm=TRUE)
#            L5_1=mean(Boot1s[1:round(n1/2)], na.rm=TRUE)
#            U5_1=mean(Boot1s[-(1:round(n1/2))], na.rm=TRUE)
#            M5_1=mean(Boot1s[ceiling(n1/4):floor(n1*0.75)], na.rm=TRUE)
#            L05_2=mean(Boot2s[1:ceiling(n2/20)])
#            U05_2=mean(Boot2s[-(1:(n2-ceiling(n2/20)))], na.rm=TRUE)
#            L5_2=mean(Boot2s[1:round(n2/2)], na.rm=TRUE)
#            U5_2=mean(Boot2s[-(1:round(n2/2))], na.rm=TRUE)
#            M5_2=mean(Boot2s[ceiling(n2/4):floor(n2*0.75)], na.rm=TRUE)
            
          ## store the results
#          temp[[idx]]<-tibble(
#            Boot=m,
#            Batch1=temp1$Batch,
#            Batch2=temp2$Batch,
#            Run1=i,
#            Run2=j,
#            meanCFU1=mean(Boot1s, na.rm=TRUE), 
#            meanCFU2=mean(Boot2s, na.rm=TRUE),
#            varCFU1=var(Boot1s, na.rm=TRUE), 
#            varCFU2=var(Boot2s, na.rm=TRUE),
#            cvCFU1=sd(Boot1s, na.rm=TRUE)/mean(Boot1s, na.rm=TRUE),
#            cvCFU2=sd(Boot2s, na.rm=TRUE)/mean(Boot2s, na.rm=TRUE),
#            Q1_1=(U05_1-M5_1)/(M5_1-L05_1), ## Hogg's stats
#            Q2_1=(U05_1-L05_1)/(U5_1-L5_1),
#            Q1_2=(U05_2-M5_2)/(M5_2-L05_2),
#            Q2_2=(U05_2-L05_2)/(U5_2-L5_2),
#            ## store p-values
#            p_sw1=sw1$p.value, 
#            p_sw2=sw2$p.value,
#            p_t=Boot.t$p.value,
#            p_w=Boot.w$p.value
#          )
#          idx<-idx+1
          ##print(idx) ## debug
#        }
#      } ## finish loop over run pairs
#    } ## finish loop over bootstraps
#  } ## end else loop
  
  ## unfold and return results
  dataSet<-dplyr::bind_rows(temp)
  if(n_conditions==1){
    return(dataSet)
  } else {
    regression_summary<-bind_rows(temp_reg)
    my_list<-list("data_summary" = dataSet, # returns summary data for pairwise run comparisons
                "regression_summary" = regression_summary) # and for regressions
    return(my_list)
  }
}
```

*summaryStats()* is a short function for returning summary statistics from a data set of counts, including the less commonly implemented Hogg's Q1 and Q2:
```{r}
summaryStats<-function(input_data, groups_in=c("Condition", "Run", "Batch"), FoldD=10, correction_constant=20){
    ## Takes a data set with columns
  ##   Run (num): Each unique index represents one data set
  ##   Count (num): Number of raw counts associated with each sample
  ##   Batch (int): batch size or weight of each sample.
  ##         Batch=1 is the minimum and indicates individual-based sampling.
  ##   D (num, optional): Fold dilution at which counts were taken
  ##         (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc)
  ##         ***IF D IS NOT GIVEN, FinalCount MUST ALREADY EXIST IN THE DATA SET***
  ##   FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling.
  ##         Will be calculated if not present.
  ## Each row of the data set represents one measurement.
  ## 
  ## Other inputs:
  ##   groups_in (char): Vector of column names to be used in group_by() for summaries
  ##   FoldD (num): Fold dilution in dilution series. Default is 10X dilutions.
  ##   correction_constant (num): a multiplier to correct the fraction of a sample used for one measurement to the original volume
  ##        (e.g. counts from 10 uL spots and an initial volume of 1 mL require correction (1000/10)=100)
  ##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  ## First - figure out what is in the data set
  ## Do we need to correct counts for dilution?
  ## Divide by batch size (may be 1) to get per-unit or per-individual numbers
  my_names<-names(input_data) 
  if(length(which(my_names=="FinalCount"))==0){  ## if FinalCount does not exist
    if(length(which(my_names=="D"))!=0){ ## if we are given a dilution factor
      input_data$FinalCount<-(correction_constant *(input_data$Count*FoldD^input_data$D))/Batch
    } else if(length(which(my_names=="D"))==0){
      stop("Either fold dilution D or FinalCount must be a column in input_data!")
    }
      else{
      input_data$FinalCount<-(correction_constant * input_data$Count)/Batch
    }
  } 
  ## now we should have final counts
  ##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  ## 
  
  data_summary<-input_data %>%
    sort_by(.$FinalCount) %>% ## arrange data in increasing order by total count
    group_by(pick(groups_in)) %>% ## depreciated idc
    summarize(n=n(),
              meanC=mean(FinalCount, na.rm=TRUE),
              medianC=median(FinalCount, na.rm=TRUE),
              varC=var(FinalCount, na.rm=TRUE),
              L05=mean(FinalCount[1:ceiling(n()/20)], na.rm=TRUE), ## get values for Hogg's calculations##
              U05=mean(FinalCount[-(1:(n()-ceiling(n()/20)))], na.rm=TRUE),
              L5=mean(FinalCount[1:round(n()/2)], na.rm=TRUE),
              U5=mean(FinalCount[-(1:round(n()/2))], na.rm=TRUE),
              M5=mean(FinalCount[ceiling(n()/4):floor(n()*0.75)], na.rm=TRUE)
              ) %>%
    mutate(
      Q1=(U05-M5)/(M5-L05),
	  	Q2=(U05-L05)/(U5-L5)
	  	)
  if (min(data_summary$n<20)){print("Warning: Hogg's statistics require n>20")}
  return(data_summary)
}
```

And a function for summarizing output when multiple experimental conditions are put into a bootstrap:
```{r}
SimBatchSummaryCorrected<-function(input_data, alpha=0.05, alpha_AIC=0.5){
  # Summarizes false positive rates for bootstrapped data with replicates
  input_data <- input_data %>%
    mutate(isSig_t=as.integer(as.logical(p.t<alpha)),
           isSig_w=as.integer(as.logical(p.w<alpha)),
           isSig_glm=as.integer(as.logical(p.glm<alpha)),
           isSig_lrt=as.integer(as.logical(p.glm.lrt<alpha)),
           isSig_glm_lrt=as.integer(as.logical(p.glm<alpha & p.glm.lrt<alpha_AIC)),
           isSig_glm_AIC=as.integer(as.logical(p.glm<alpha & p.glm.AIC<alpha_AIC)),
           isSig_glm_AICu=as.integer(as.logical(p.glm<alpha & p.glm.AICu<alpha_AIC)),
           isSig_aov=as.integer(as.logical(p.aov<alpha))
    )
  input_data_testsummary<-input_data %>%
    group_by(batch) %>%
    summarize(countSigT=sum(isSig_t),
              countSigW=sum(isSig_w),
              countSigGLM=sum(isSig_glm),
              countSigGLMLRT=sum(isSig_glm_lrt),
              countSigGLMAIC=sum(isSig_glm_AIC),
              countSigGLMAICu=sum(isSig_glm_AICu),
              countSigAOV=sum(isSig_aov),
              n=n()) %>%
    mutate(fracSigT=countSigT/n,
           fracSigW=countSigW/n,
           fracSigGLM=countSigGLM/n,
           fracSigGLMLRT=countSigGLMLRT/n,
           fracSigGLMAIC=countSigGLMAIC/n,
           fracSigGLMAICu=countSigGLMAICu/n,
           fracSigAOV=countSigAOV/n,
    ) %>%
    ungroup() %>%
    dplyr::select(batch, fracSigT, fracSigW, fracSigGLM,
                  fracSigGLMLRT, fracSigGLMAIC, fracSigGLMAICu, fracSigAOV) %>%
    pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
                 names_prefix = "fracSig", values_to = "fracSig")
  return(input_data_testsummary)
}
```


## Simulation-Based Thresholding: If You Have Individual-Based Data for One Experimental Condition
To assess run to run variation and false positive rates from individual-based data, you will need data from at least two independent runs (not internal/technical replicates) of the same experiment. Ideally each run will contain a sufficient number of data points to allow reasonable bootstrapping; for worms, we strongly recommend a minimum sample size of 18 individuals.  

If individual-based data are available, it is possible to use bootstrapping to empirically assess the probable effects of biological averaging. The workflow here will recapitulate the results in Figure 1 of the manuscript; users should replace our data (bacterial load for *Salmonella enterica* in N2 worms) with their own data set.

First, load in the data. In this example, we use data from the manuscript, focusing on single-worm data for bacterial load of *Salmonella enterica* LT2 (SE).

```{r}
## Example data - replace with your own
SaSeCountAll<-read_xlsx("SaSeCount.xlsx")  ## full data file including single worm data
input_data_1<-SaSeCountAll %>%
  dplyr::filter(Condition=="SE" & Batch==1) ## filter data to isolate only S. enterica single-worm data
```

The bootstrapping functions expect count-based data with a particular structure. Each row of the data set represents one measurement, and the data should have columns: 
  * Run (num): Each unique index represents one data set
  * Count (num): Number of raw counts associated with each sample
  * Batch (int): batch size or weight of each sample. Batch=1 is the minimum and indicates individual-based sampling.
  * D (num): Fold dilution at which counts were taken (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc) ***NEEDED FOR BOOTSTRAPPING ON RAW COUNTS***
  * FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling. (This will be calculated if not present, if D is provided. Otherwise an error is returned.)  
  
Let's make sure the data structure is OK before proceeding.
```{r}
glimpse(input_data_1) ## take a quick look at the data object contents and confirm column names are OK
```
The resulting data set contains three independent runs of individual-based data, all of which are replicates of the same experimental condition (colonization of N2 adults by *S. enterica*). 
```{r}
unique(input_data_1$Run)
```

A quick plot is a useful check at this point:
```{r}
input_data_1 %>%
  ggplot(aes(x=factor(Run), y=logCFU, color=factor(Run))) + ## plot bacterial load for each experiment (run)
  geom_jitter(shape=16, position=position_jitter(0.05)) +   ## with a small jitter for visibility
  geom_violin(fill=NA) +    ## and a violin silhouette to see density more easily
  ylim(-0.1,6)+  ## y limits help to keep the plot nice but aren't really needed here
  theme_classic() + ## overall theme
  theme(
    text=element_text(size=xTextSize), 
    axis.title.x = element_blank(), 
    plot.title=element_text(hjust=0.5),
    legend.position = "none") + 
  labs(title=expression(italic("S. enterica")), y=expression(log[10](CFU/Worm)))
```
We can also generate summary statistics for the data using the *summaryStats()* function defined in the last section:
```{r}
myStats<-summaryStats(input_data = input_data_1)
myStats
```


If the data look as expected, we can proceed to bootstrap. It may be smart to do one run of a bootstrap as a sanity check, to get a feel for what the results will look like. 
```{r}
## Pull out two runs from the data set
temp1<-input_data_1 %>%
  dplyr::filter(Condition=="SE" & Run=="3")
temp2<-input_data_1 %>%
  dplyr::filter(Condition=="SE" & Run=="2")

## For each run, generate bootstrapped data
## Here we are allowing defaults including batch_sizes=c(1, 5, 10, 20, 50)
Boot1<-bootOnCounts(n_reps=dim(temp1)[1], mydata=temp1)
Boot2<-bootOnCounts(n_reps=dim(temp2)[1], mydata=temp2)

## Name the runs something useful
Boot1$Run<-as.factor("Run3")
Boot2$Run<-as.factor("Run2")

## and combine into a single object for easy plotting
jointBoot<-rbind(Boot1, Boot2)

## easy plotting
jointBoot %>%
	ggplot(aes(x=factor(Run), y=logCFU, color=factor(Run))) + 
	geom_jitter(shape=16, position=position_jitter(0.05)) +
	geom_violin(fill=NA) + 
  geom_hline(yintercept=log10(20), color="black", lty="dashed")+ ## TOD for example data - replace with your own
	ylim(-0.1,6.2) +   ## Reasonable limits for CFU/worm in log10 scale - replace with your own
  theme_classic() +
  scale_color_viridis_d(begin=0.3, end=0.8) +
	theme(
	  text=element_text(size=xTextSize), 
	  axis.title.x = element_blank(),
	  axis.text.x = element_blank(),
	  axis.text.y = element_text(size=xTextSize-1),
	  axis.title.y = element_text(size=xTextSize),
	  plot.title=element_text(hjust=0.5,size=xTextSize),
	  legend.title = element_blank(),
	  legend.text = element_text(size=xTextSize-1)
	  )+
  facet_wrap(vars(Batch), ncol=5)+
  labs(title="Simulated Batch Data", 
       y=expression(log[10]("Total Count")), x="Run")+
  stat_compare_means(method="t.test", label.y = 6.2, size=2.5)+   ## label positions may need adjustment for your data
  stat_compare_means(method="wilcox.test", label.y=5.9, size=2.5)
```

If you are satisfied that your data look ok, and that the bootstrap is proceeding correctly, you can proceed to generate a large number of bootstrap data sets and calculate the associated statistics of interest. The function *bootonCountsStats()* will do this, returning summary statistics for bootstrapped data and between-group comparisons:
```{r}
## Carries out nboot simulations and stores the output
# Make sure that your values for FoldD and correction_constant are consistent with the data!
BootCombinations1<-bootOnCountsStats(input_data=input_data_1, batch_sizes=c(1,5,10,20,50), nboot=1000, 
                                      FoldD=10, correction_constant=20)
## If you want more human-interpretable run IDs, they must be added after the bootstrap
BootCombinations1$Run1ID<-paste("Run", BootCombinations1$Run1, sep=" ")
BootCombinations1$Run2ID<-paste("Run", BootCombinations1$Run2, sep=" ")

## look at contents
glimpse(BootCombinations1)
```

Get and plot the fraction of runs in which the pairwise test indicates a significant difference between groups. Comparisons of each data set to itself should always have false positive rates close to the declared value alpha.
```{r}
## how many comparisons are significant for each test?
BootCombinations1<-BootCombinations1 %>%
  mutate(isSig_t=as.integer(as.logical(p.t<0.05)), ## label significant t-tests
         isSig_w=as.integer(as.logical(p.w<0.05)), ## and significant Wilcoxon tests
         RunPair=paste(Run1ID, "v", Run2ID, sep=" ")) ## and label each result with the pairs of runs compared

## Summarize to get fraction of bootstrap replicates where each test is significant
## for each pair of runs at each batch size
BootCombinations1_testsummary<-BootCombinations1 %>%
  group_by(RunPair, Batch) %>%
  summarize(countSigT=sum(isSig_t),
            countSigW=sum(isSig_w),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair, Batch, fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")

## plot
BootCombinations1_testsummary %>%
  ggplot(aes(x=factor(Batch), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5)
  )+
  labs(x="Batch Size", y="Fraction Significant", fill="Test",
       title="Pairwise Tests")+
  facet_wrap(~RunPair)
```

Having estimated the effects of batching on error rates in pairwise comparisons, we next want to estimate the magnitude of run-to-run variation. This will determine whether thresholding comparisons based on a minimum empirical effect size will correct false-positive inflation due to batching. 

We can do this empirically using the measures of run-to-run distance from Figure 2. First calculate these distances:
```{r}
BootCombinations1 <- BootCombinations1 %>%
  mutate(
    		meanCFUdist=abs(meanCFU1-meanCFU2)/(meanCFU1+meanCFU2),
    		meanCFUCohenD=abs(meanCFU1-meanCFU2)/sqrt((varCFU1+varCFU2)/2),
    		cvdist=abs(cvCFU1-cvCFU2),
    		Q1dist=abs(Q1.1-Q1.2),
    		Q2dist=abs(Q2.1-Q2.2)
  )
```

And plot out:
```{r}
my.labs<-c("Mean CFU", "Cohen's D", "CV", "Q1", "Q2")
names(my.labs)=c("meanCFUdist", "meanCFUCohenD", "cvdist", "Q1dist","Q2dist")
BootCombinations1 %>%
  pivot_longer(., cols = c(meanCFUdist, meanCFUCohenD, cvdist, Q1dist, Q2dist), 
               names_to = "Var", values_to = "Value") %>%
  mutate(Var=factor(Var, 
                    levels=c("meanCFUdist", "meanCFUCohenD", "cvdist", "Q1dist", "Q2dist")) ) %>%
  ggplot(aes(x=factor(Batch), y=Value, color=Var))+
  geom_jitter(shape=16, position=position_jitter(0.05)) +
  geom_boxplot(fill=NA) +	
  theme_classic() + 
  scale_color_viridis_d(option="magma", begin=0.3, end=0.7)+
  theme( 
    axis.text.x = element_text(size=xTextSize-2),
    axis.title.y = element_text(size=xTextSize), 
    axis.text.y = element_text(size=xTextSize-2), 
    legend.position="none", 
      plot.title=element_text(hjust=0.5, size=xTextSize)) +
  labs(title="", y="Distance between pairs", x="Batch Size")+
  facet_wrap(vars(Var), scales="free", nrow=1, labeller=labeller(Var=my.labs))
```

Note that while the quantile range of rescaled distances between means is relatively unaffected by batching, Cohen's D (variance-normalized mean distance) and distances in the coefficient of variation (CV) and in Q1 (quantile-based measure of skewness) are strongly affected by batching, consistent with the observed increase in false-positive pairwise comparisons.  

We can get exact numbers from the quantiles of the distribution of mean distances:
```{r}
BootCombinations1 %>%
  group_by(Batch) %>%
  summarize(q75MeanDist=quantile(meanCFUdist, probs=0.75),
            q90MeanDist=quantile(meanCFUdist, probs=0.9))
```

The distribution of mean distances suggests a cutoff of 0.3-0.35, below which we will not consider any statistically significant comparisons to be "real". If we impose this cutoff, we find that implementing this cutoff dramatically reduces - and for some pairs entirely eliminates - false positive rate inflation due to batching.
```{r}
## how many comparisons are significant for each test?
meandiff_cutoff_1<-0.35

BootCombinations1_cutoff<-BootCombinations1 %>%
  mutate(isSig_tc=as.integer(as.logical(p.t<0.05 & meanCFUdist>meandiff_cutoff_1)), ## label significant t-tests
         isSig_wc=as.integer(as.logical(p.w<0.05 & meanCFUdist>meandiff_cutoff_1)) ## and significant Wilcoxon tests
         ) ## and label each result with the pairs of runs compared 
glimpse(BootCombinations1_cutoff) ## make sure the data are as expected
```

```{r}
## Summarize to get fraction of bootstrap replicates where each test is significant
## for each pair of runs at each batch size
BootCombinations1_cutoff_testsummary<-BootCombinations1_cutoff %>%
  group_by(RunPair, Batch) %>%
  summarize(countSigT=sum(isSig_tc),
            countSigW=sum(isSig_wc),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair, Batch, fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")

## plot
BootCombinations1_cutoff_testsummary %>%
#  dplyr::filter(RunPair %in% pair_list) %>%
  ggplot(aes(x=factor(Batch), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  ylim(0,1)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5)
  )+
  labs(x="Batch Size", y="Fraction Significant", fill="Test",
       title="Pairwise Tests, Thresholded")+
  facet_wrap(~RunPair)
```

Keep in mind that the decision of how large a distance to accept as "real", much like the decision to accept a particular level of alpha as "significant", is somewhat arbitrary and can depend on the question being asked. The usual and obvious tradeoff applies - if you want to impose a more stringent threshold to minimize false positive rates, you run the risk of decreasing true-positive discovery.  



***
## Simulation-Based Thresholding: If You Have Individual-Based Data for More Than One Experimental Condition

If you have a second set of individual-based data from a condition that you think is "different", you can use within AND between-group differences to balance these risks. For example, the data set from the manuscript also contains single-worm data for colonization by *Staphylococcus aureus* (SA) that can be used for comparisons.

```{r}
input_data_1<-SaSeCountAll %>%
  dplyr::filter(Condition=="SE" & Batch==1) ## filter data to isolate only S. enterica single-worm data

my_runs<-c(4,5,6) ## The last three runs with SA have a TOD similar to those in the SE data and are more comparable
input_data_2<-SaSeCountAll %>%
  dplyr::filter(Condition=="SA"& Batch==1) %>% ## filter data to isolate only S. aureus single-worm data
  dplyr::filter(Run %in% my_runs)    ## from the last three runs of this experiment

# Relabel runs so that Run is an internal index
input_data_2$Run[input_data_2$Run==4]<-1
input_data_2$Run[input_data_2$Run==5]<-2
input_data_2$Run[input_data_2$Run==6]<-3

input_data<-rbind(input_data_1, input_data_2) ## and combine with SE data

glimpse(input_data)
```
Make sure data structure and run indexing are correct:
```{r}
unique(input_data$Batch)
unique(input_data$Run)
```
As always, it's a good idea to plot out and make sure the data look OK before proceeding:
```{r}
input_data %>%
  ggplot(aes(x=factor(Run), y=logCFU, color=factor(Run))) + ## plot bacterial load for each experiment (run)
  geom_jitter(shape=16, position=position_jitter(0.05)) +   ## with a small jitter for visibility
  geom_violin(fill=NA) +    ## and a violin silhouette to see density more easily
  ylim(-0.1,6)+  ## y limits help to keep the plot nice but aren't really needed here
  theme_classic() + ## overall theme
  theme(
    text=element_text(size=xTextSize), 
    axis.title.x = element_blank(), 
    plot.title=element_text(hjust=0.5),
    legend.position = "none") + 
  labs(y=expression(log[10](CFU/Worm))) +
  facet_wrap(~Condition, scales="free_x")
```

Because more than one experimental condition is present, we have the opportunity to use regression-based partition of variation (maybe ANOVA, but more likely a GLM is appropriate) to directly examine the relative contributions of variation among replicates and variation between conditions to the overall variation in the data set.

For an ANOVA to be valid, we need to know that the data are homoscedastic (equal variances):
```{r}
# Levene's test is commonly used to assess heteroscedasticity
```

And ideally that the data are normally distributed (although ANOVA is fairly robust to this assumption):
```{r}
# Shapiro-Wilk is a common test for normality, but there are others
# For example, Koromogorov-Smirnov is sometimes used, but this test is very sensitive to sample size

```

Let's assume for a minute that these tests show that our data are homoscedastic and normally distributed, so we are completely comfortable applying an ANOVA. We will bootstrap this to assess significance rates, but before we decide to rely on these results, it is a good idea to run ANOVA on the observed data:
```{r}
input_data.aov<-input_data %>%
  mutate(Run=as.factor(Run)) %>%
  aov(logCFU~Condition+Run, data=.)
summary(input_data.aov)
```

And check the usual metrics for appropriateness of fit:
```{r}
plot(input_data.aov)
```

For these data, residual vs fitted plots aren't terrible, but the QQ plot is bad, indicating marked non-normality of the residuals. ANOVA probably isn't the best choice on these data. (The bootstrap function will report ANOVA significance rates for any input data; the user needs to determine whether or not to use these results.)

Much of the time, GLM will be a better choice. For count data without significant zero contamination or censoring, the Gamma family with the default logit link is often suitable, but this will depend on the properties of your data. 

Note that to use Gamma, any zeros in the data set will need to be replaced with some non-zero value. Here, we will replace these values with a small positive number; another common choice is to choose some formula to replace zeros with values between 0 and the threshold of detection. If zeros are rare, the choice of adjustment will have very little effect on the results of this kind of analysis.
```{r}
# adjust zeros by simple replacement
input_data$logCFU[input_data$logCFU==0]<-0.1

# put through GLM
input_data.glm<-input_data %>%
  mutate(Run=as.factor(Run)) %>%
  glm(logCFU~Condition+Run, family=Gamma, data=.)
# print results to screen
summary(input_data.glm)
```

As with the ANOVA, it is a good idea to check the fit before deciding that the choice of model is appropriate.
```{r}
# deviance density plot - should be one relatively smooth curve with a mean close to 0
# (the zero correction may introduce a "bump" somewhere depending on how it is implemented)
plot(density(resid(input_data.glm, type='deviance'))) 
# all deviances - should be a basically straight line, no curves
scatter.smooth(1:length(rstandard(input_data.glm, type='deviance')), rstandard(input_data.glm, type='deviance'), col='gray')
#residuals vs fitted - should be a basically straight line
scatter.smooth(predict(input_data.glm, type='response'), 
               rstandard(input_data.glm, type='deviance'), col='gray')
```

We can carry out the same bootstrap analysis as before, this time allowing comparisons within each condition (runs 1-3, SE; runs 4-6, SA) as well as across conditions, to determine the range and distribution of distances.
```{r}
## Carries out nboot simulations and stores
BootCombinations2<-bootOnCountsStats(input_data=input_data, batch_sizes=c(1,5,10,20,50), nboot=1000, 
                                      FoldD=10, correction_constant=20)
#### look at contents
glimpse(BootCombinations2)
```
Since we have two experimental conditions, the function now returns two objects:
```{r}
# a summary of the run-by run comparisons
BootCombinations2_ds<-BootCombinations2$data_summary

# and a summary of the comparisons across experimental conditions (here, S. enterica vs S. aureus)
BootCombinations2_rs<-BootCombinations2$regression_summary

# remove the parent object
rm(BootCombinations2)

#### If you want more human-interpretable run IDs, they must be added after the bootstrap
BootCombinations2_ds$Run1ID<-paste(BootCombinations2_ds$Condition1, "Run", BootCombinations2_ds$Run1, sep=" ")
BootCombinations2_ds$Run2ID<-paste(BootCombinations2_ds$Condition2, "Run", BootCombinations2_ds$Run2, sep=" ")

#### look at contents of the first object (same as when we had only one experimental condition)
glimpse(BootCombinations2_ds)
```

As before, we can now get and plot the fraction of runs in which the pairwise test indicates a significant difference between groups. Comparisons of each data set to itself should always have false positive rates close to the declared value alpha.
```{r}
#### how many comparisons are significant for each test?
BootCombinations2_ds<-BootCombinations2_ds %>%
  mutate(isSig_t=as.integer(as.logical(p.t<0.05)), ## label significant t-tests
         isSig_w=as.integer(as.logical(p.w<0.05)), ## and significant Wilcoxon tests
         RunPair=paste(Condition1, Run1, "v", Condition2, Run2, sep=" ")) ## and label each result with the pairs of runs compared

## Summarize to get fraction of bootstrap replicates where each test is significant
## for each pair of runs at each batch size
BootCombinations2_testsummary<-BootCombinations2_ds %>%
  group_by(RunPair, Batch) %>%
  summarize(countSigT=sum(isSig_t),
            countSigW=sum(isSig_w),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair, Batch, fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")
```

```{r fig.width=10}
## plot
BootCombinations2_testsummary %>%
  ggplot(aes(x=factor(Batch), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5)
  )+
  labs(x="Batch Size", y="Fraction Significant", fill="Test",
       title="Pairwise Tests")+
  facet_wrap(~RunPair, ncol=7)
```

Now we can calculate distances and label by whether pairs of runs are from the same experimental condition.
```{r}
BootCombinations2_ds <- BootCombinations2_ds %>%
  mutate(
    		meanCFUdist=abs(meanCFU1-meanCFU2)/(meanCFU1+meanCFU2),
    		meanCFUCohenD=abs(meanCFU1-meanCFU2)/sqrt((varCFU1+varCFU2)/2),
    		cvdist=abs(cvCFU1-cvCFU2),
    		Q1dist=abs(Q1.1-Q1.2),
    		Q2dist=abs(Q2.1-Q2.2),
    		PairType="Different"
  )
## correct Pair to indicate whether runs are from the same condition
my_size<-dim(BootCombinations2_ds)[1] # size of object
for(i in seq_len(my_size)){
  if(BootCombinations2_ds$Condition1[i]==BootCombinations2_ds$Condition2[i]) {
    BootCombinations2_ds$PairType[i]="Same"
  } 
}
glimpse(BootCombinations2_ds)
```

And plot out (leaving out Q1, since it is very noisy):
```{r fig.height=8}
my.labs<-c("Mean CFU", "Cohen's D", "CV", "Q2")
names(my.labs)=c("meanCFUdist", "meanCFUCohenD", "cvdist", "Q2dist")

BootCombinations2_ds %>%
  dplyr::select(Batch, RunPair, PairType, meanCFUdist, meanCFUCohenD, cvdist, Q2dist) %>%
  pivot_longer(., cols = c(meanCFUdist, meanCFUCohenD, cvdist, Q2dist), 
               names_to = "Var", values_to = "Value") %>%
  mutate(Var=factor(Var, 
                    levels=c("meanCFUdist", "meanCFUCohenD", "cvdist", "Q2dist")) ) %>%
  ggplot(aes(x=factor(PairType), y=Value, color=Var))+
  geom_jitter(shape=16, position=position_jitter(0.05)) +
  geom_boxplot(fill=NA) +	
  theme_classic() + 
  scale_color_viridis_d(option="magma", begin=0.3, end=0.7)+
  theme( 
    axis.text.x = element_text(size=xTextSize-2),
    axis.title.y = element_text(size=xTextSize), 
    axis.text.y = element_text(size=xTextSize-2), 
    legend.position="none", 
      plot.title=element_text(hjust=0.5, size=xTextSize)) +
  labs(title="", y="Distance between pairs", x="Pair Type")+
  facet_grid(vars(Batch), vars(Var), scales="free", labeller=labeller(Var=my.labs))
```

We can get exact numbers from the quantiles of the distribution of mean distances:
```{r}
BootCombinations2_ds %>%
  group_by(PairType, Batch) %>%
  summarize(q75MeanDist=quantile(meanCFUdist, probs=0.75),
            q90MeanDist=quantile(meanCFUdist, probs=0.9))
```

We can then implement a cutoff:
```{r}
## how many comparisons are significant for each test?
meandiff_cutoff<-0.35

BootCombinations2_ds<-BootCombinations2_ds %>%
  mutate(isSig_tc=as.integer(as.logical(p.t<0.05 & meanCFUdist>meandiff_cutoff)), ## label significant t-tests
         isSig_wc=as.integer(as.logical(p.w<0.05 & meanCFUdist>meandiff_cutoff)) ## and significant Wilcoxon tests
         ) ## and label each result with the pairs of runs compared 
glimpse(BootCombinations2_ds) ## make sure the data are as expected
```

Plotting this out, we see that false positive rates are well controlled for the most part, but that run 2 (SE) becomes essentially indistinguishable from any of the SA data sets (runs 4-6), and run 6 (SA) becomes indistinguishable from any of the SE data sets (runs 1-3), reflecting the tradeoff in false negative rates.
```{r, fig.width=10}
## Summarize to get fraction of bootstrap replicates where each test is significant
## for each pair of runs at each batch size
BootCombinations2_cutoff_testsummary<-BootCombinations2_ds %>%
  group_by(RunPair, PairType, Batch) %>%
  summarize(countSigT=sum(isSig_tc),
            countSigW=sum(isSig_wc),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair, PairType, Batch, fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")

## plot
BootCombinations2_cutoff_testsummary %>%
  ggplot(aes(x=factor(Batch), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5)
  )+
  labs(x="Batch Size", y="Fraction Significant", fill="Test",
       title="Pairwise Tests, Thresholded")+
  facet_wrap(~RunPair, ncol=7)
```

Since the data contained more than one experimental conditions, we can also explore what happens when bootstrapped data are compared across experimental conditions. The second object that was output from the bootstrap simulations contains this information:
```{r}
# A summary of the comparisons across experimental conditions (here, S. enterica vs S. aureus)
glimpse(BootCombinations2_rs)
```

Along with pairwise tests, this object includes comparisons of nested GLMs:
* **p.glm** is the p-value associated with experimental condition ("Set" in this output) in a gamma-family GLM with formula log(CFU) ~ Set + Run
* **p.aov** is the p-value associated with an ANOVA of the same form. Since ANOVA will rarely be the best choice for comparisons of population counts, this is shown largely for comparison.
* **p.glm.lrt** is the p-value for a likelihood ratio test comparing the two-term GLM to a nested GLM of formula log(CFU) ~ Set
* **p.glm.AIC** is the result of comparing these two models based on AIC
* **p.glm.AICu** is the result of comparing the additive model with a full GLM of the form log(CFU) ~ Set * Run 

Let's summarize these results:
```{r}
BootCombinations2_rs_summary<-SimBatchSummaryCorrected(BootCombinations2_rs)
glimpse(BootCombinations2_rs_summary)
```

And plot:
```{r}
BootCombinations2_rs_summary %>%
  ggplot(aes(x=factor(batch), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5)
  )+
  labs(x="Batch Size", y="Fraction Significant", fill="Test",
       title="Test Results")
```


***
## Simulation-Based Thresholding: If You Do Not Have Individual-Based Data

### If You Have Biologically Averaged Data for One Condition
To assess run to run variation and false positive rates from batch-based data, you will again need data from at least two independent runs (not internal/technical replicates) of the same experiment. Ideally each run will contain a sufficient number of data points to allow reasonable bootstrapping; for worms, we strongly recommend a minimum sample size of 18 individuals.

As individual-based data are unavailable, we cannot infer the distributions of individuals within each sample, and we therefore cannot directly simulate the effects of changing batch size. However, as simulations from individual-based data indicate that the distribution of rescaled mean distances should depend only weakly on batching, we can use that distribution to obtain a threshold value.

As the data set used in this manuscript does not contain batch-based data with the correct structure, we will instead demonstrate this workflow with simulated data. DO NOT RUN THIS BLOCK IF USING REAL DATA - FOR ILLUSTRATION ONLY.
```{r}
SE_data<-SaSeCountAll %>%
  dplyr::filter(Condition=="SE" & Batch==1)
## Pull out runs from the data set
temp1<-SE_data %>%
  dplyr::filter(Run=="1")
temp2<-SE_data %>%
  dplyr::filter(Run=="2")
temp3<-SE_data %>%
  dplyr::filter(Run=="3")

## For each run, generate bootstrapped data
## Here we are allowing defaults apart from batch sizes (only need one size)
Boot1<-bootOnCounts(n_reps=dim(temp1)[1], batch_sizes=5, mydata=temp1)
Boot2<-bootOnCounts(n_reps=dim(temp2)[1], batch_sizes=5, mydata=temp2)
Boot3<-bootOnCounts(n_reps=dim(temp3)[1], batch_sizes=5, mydata=temp3)

## restore the dilutions from plating data
Boot1$D<-temp1$D
Boot2$D<-temp2$D
Boot3$D<-temp3$D

## Name the runs something useful
Boot1$Run<-as.factor("Run1")
Boot2$Run<-as.factor("Run2")
Boot3$Run<-as.factor("Run3")

## combine into a single object for easy plotting
input_data_batch<-rbind(Boot1, Boot2, Boot3) 

correction_constant<-20
## and reverse FinalCount to get raw counts
input_data_batch$Count<-round(10^(log10(input_data_batch$FinalCount/correction_constant) -input_data_batch$D))
```

If you are using your own data, read that in instead.
```{r}
##input_data_batch<-read_xlsx("input_data_batch.xlsx")  ## replace with your data file
```

The bootstrapping functions expect count-based data with a particular structure. Each row of the data set represents one measurement, and the data should have columns: 
  * Run (num): Each unique index represents one data set
  * Count (num): Number of raw counts associated with each sample
  * Batch (int): batch size or weight of each sample. Batch=1 is the minimum and indicates individual-based sampling.
  * D (num): Fold dilution at which counts were taken (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc) 
  * FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling. (This will be calculated if not present, if D is provided. Otherwise an error is returned.)
  
Let's make sure the data structure is OK before proceeding. In this case, we should have three runs' worth of data, all from batches of 5 individuals per data point.
```{r}
glimpse(input_data_batch) ## take a quick look at the data object contents and confirm column names are OK
view(input_data_batch) # show the whole data frame in a new tab
```

And as always it is a good idea to plot out to make sure the data look ok:
```{r}
## easy plotting
input_data_batch %>%
	ggplot(aes(x=factor(Run), y=logCFU, color=factor(Run))) + 
	geom_jitter(shape=16, position=position_jitter(0.05)) +
	geom_violin(fill=NA) + 
  geom_hline(yintercept=log10(20), color="black", lty="dashed")+ ## TOD for our plating data - replace with your own
	ylim(-0.1,6.2) + 
  theme_classic() +
  scale_color_viridis_d(begin=0.3, end=0.8) +
	theme(
	  text=element_text(size=xTextSize), 
	  axis.title.x = element_blank(),
	  axis.text.x = element_blank(),
	  axis.text.y = element_text(size=xTextSize-1),
	  axis.title.y = element_text(size=xTextSize),
	  plot.title=element_text(hjust=0.5,size=xTextSize),
	  ##legend.position.inside=c(0.9,0.3),
	  legend.title = element_blank(),
	  legend.text = element_text(size=xTextSize-1)
	  )+
  labs(title="Batch Data", 
       y=expression(log[10]("Count Per Individual")), x="Run")
```

The function used for bootstrapping individual-based data in the previous section will also bootstrap batch-based data from raw counts and dilution series information, again assuming sampling noise on counts. However, since the variances of the individual data are not known, in this case we can only simulate bootstrap distributions with the same amount of biological averaging as in the data.
```{r}
## Carries out nboot simulations and stores
BootCombinations_batch<-bootOnCountsStats(input_data=input_data_batch, nboot=1000, 
                                      FoldD=10, correction_constant=20)
## If you want more human-interpretable run IDs, they must be added after the bootstrap
BootCombinations_batch$Run1ID<-paste("Run", BootCombinations_batch$Run1, sep=" ")
BootCombinations_batch$Run2ID<-paste("Run", BootCombinations_batch$Run2, sep=" ")

## look at contents
glimpse(BootCombinations_batch)
```
We can see the fraction of significant tests without adjustment for threshold. As before, comparisons of each data set to itself should always have false positive rates close to the declared value alpha.
```{r}
## how many comparisons are significant for each test?
BootCombinations_batch<-BootCombinations_batch %>%
  mutate(isSig_t=as.integer(as.logical(p_t<0.05)), ## label significant t-tests
         isSig_w=as.integer(as.logical(p_w<0.05)), ## and significant Wilcoxon tests
         RunPair=paste(Run1ID, "v", Run2ID, sep=" ")) ## and label each result with the pairs of runs compared

## Summarize to get fraction of bootstrap replicates where each test is significant
## for each pair of runs at each batch size
BootCombinations_batch_testsummary<-BootCombinations_batch %>%
  group_by(RunPair) %>%
  summarize(countSigT=sum(isSig_t),
            countSigW=sum(isSig_w),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair, fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")

## Remove redundant pairs ('Run 1 v Run 3' is the same as 'Run3 v Run 1')
## This list works if you have three runs -
## change your list accordingly
pair_list=c("Run 1 v Run 1", "Run 2 v Run 2", "Run 3 v Run 3",
            "Run 1 v Run 2","Run 1 v Run 3","Run 2 v Run 3")
## plot
BootCombinations_batch_testsummary %>%
  dplyr::filter(RunPair %in% pair_list) %>%
  ggplot(aes(x=factor(RunPair), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  ylim(0,1)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5),
    axis.text.x = element_text(angle = 90, vjust = 1, hjust=1)
  )+
  labs(x="", y="Fraction Significant", fill="Test",
       title="Pairwise Tests")
```

As with the individual-based data bootstraps, we can empirically set thresholds using the measures of run-to-run distance from Figure 2. First calculate these distances:
```{r}
BootCombinations_batch <- BootCombinations_batch %>%
  mutate(
    		meanCFUdist=abs(meanCFU1-meanCFU2)/(meanCFU1+meanCFU2),
    		meanCFUCohenD=abs(meanCFU1-meanCFU2)/sqrt((varCFU1+varCFU2)/2),
    		cvdist=abs(cvCFU1-cvCFU2),
    		Q1dist=abs(Q1_1-Q1_2),
    		Q2dist=abs(Q2_1-Q2_2)
  )
```

And plot out:
```{r fig.width=10}
my.labs<-c("Mean CFU", "Cohen's D", "CV", "Q1", "Q2")
names(my.labs)=c("meanCFUdist", "meanCFUCohenD", "cvdist", "Q1dist","Q2dist")
BootCombinations_batch %>%
  pivot_longer(., cols = c(meanCFUdist, meanCFUCohenD, cvdist, Q1dist, Q2dist), 
               names_to = "Var", values_to = "Value") %>%
  mutate(Var=factor(Var, 
                    levels=c("meanCFUdist", "meanCFUCohenD", "cvdist", "Q1dist", "Q2dist")) ) %>%
  ggplot(aes(x=factor(RunPair), y=Value, color=Var))+
  geom_jitter(shape=16, position=position_jitter(0.05)) +
  geom_boxplot(fill=NA) +	
  theme_classic() + 
  scale_color_viridis_d(option="magma", begin=0.3, end=0.7)+
  theme( 
    axis.text.x = element_text(angle = 90, vjust = 1, hjust=1),
    legend.position="none", 
      plot.title=element_text(hjust=0.5)) +
  labs(title="", y="Distance between pairs", x="Batch Size")+
  facet_wrap(vars(Var), scales="free", nrow=1, labeller=labeller(Var=my.labs))
```

We can get exact numbers from the quantiles of the distribution of mean distances:
```{r}
BootCombinations_batch %>%
  group_by(RunPair) %>%
  summarize(q75MeanDist=quantile(meanCFUdist, probs=0.75),
            q90MeanDist=quantile(meanCFUdist, probs=0.9))
```

The distribution of mean distances suggests a cutoff of ~0.4, below which we will not consider any statistically significant comparisons to be "real". If we impose this cutoff, we find that implementing this cutoff dramatically reduces - and for some pairs entirely eliminates - false positive rate inflation due to batching.
```{r}
## how many comparisons are significant for each test?
meandiff_cutoff<-0.4

BootCombinations_batch_cutoff<-BootCombinations_batch %>%
  mutate(isSig_tc=as.integer(as.logical(p_t<0.05 & meanCFUdist>meandiff_cutoff)), ## label significant t-tests
         isSig_wc=as.integer(as.logical(p_w<0.05 & meanCFUdist>meandiff_cutoff)) ## and significant Wilcoxon tests
         ) ## and label each result with the pairs of runs compared 
glimpse(BootCombinations_batch_cutoff)

## Summarize to get fraction of bootstrap replicates where each test is significant
## for each pair of runs at each batch size
BootCombinations_batch_cutoff_testsummary<-BootCombinations_batch_cutoff %>%
  group_by(RunPair) %>%
  summarize(countSigT=sum(isSig_tc),
            countSigW=sum(isSig_wc),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair,fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")

## Remove redundant pairs ('Run 1 v Run 3' is the same as 'Run3 v Run 1')
## This list works if you have three runs -
## change your list accordingly
pair_list=c("Run 1 v Run 1", "Run 2 v Run 2", "Run 3 v Run 3",
            "Run 1 v Run 2","Run 1 v Run 3","Run 2 v Run 3")
## plot
BootCombinations_batch_cutoff_testsummary %>%
  dplyr::filter(RunPair %in% pair_list) %>%
  ggplot(aes(x=factor(RunPair), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  ylim(0,1)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5),
    axis.text.x = element_text(angle = 90, vjust = 1, hjust=1)
  )+
  labs(x="", y="Fraction Significant", fill="Test",
       title="Pairwise Tests, Thresholded")
```


## If You Have Biologically Averaged Data In A Factorial Design With More Than One Condition: Comparing Within- and Between-Group Variance


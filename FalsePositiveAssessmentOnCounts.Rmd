---
title: "False Positive Assessment On Counts"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

# Purpose
This notebook accompanies the code in the corresponding manuscript "Sample pooling inflates error rates in between-sample comparisons: an empirical investigation of the statistical properties of count-based data". The code contained here is intended to allow end users to interrogate their own count-based data using the approaches in that manuscript.

This code was written and run in R 4.4.0 running in Rstudio 2024.04.0 build 735.


# Housekeeping and Functions
Before we do anything, we will need to load the packages this code depends on, and also enter the custom functions that we will use to interrogate the data.

First, load the required packages, using PACkageMANager to install any that are missing.
```{r}
#install.packages("pacman") # Uncomment if pacman is not already installed
pacman::p_load(ggplot2, tidyverse, cowplot, mclust, e1071, ggpubr, ggpmisc, readxl, patchwork, sBIC)
# and place a global
xTextSize<-14
```

Now we will create some functions that will be needed below. Make sure to run these code blocks before proceeding to the next section.

First function provides the basic functionality to boostrap on raw count data, which may or may not be from serially diluted samples:
```{r}
bootOnCounts<-function(n_reps, mydata, batch_sizes=c(1,5,10,20,50), FoldD=10, correction_constant=20){
  # Expects a number of replicates for the bootstrap (n_reps)
  # and a data frame where each row represents one individual (mydata)
  # where the number of colonies counted is in column "Count"
  # the dilution at which these colonies were measured is in column "D",
  #   e.g. D=0 for undiluted sample, D=1 for the first FoldD-fold dilution, etc
  # FoldD(num) is fold dilution in dilution series 
  #  (Default is 10X dilutions, e.g. each step in serial dilution is 1:10 volume)
  #
  ## The dilution correction factor (numeric) is given as "correction_constant"
  # and is the ratio of the original volume and the volume plated/measured
  # e.g. for 10 uL spots and an original volume of 1 mL, correction_constant = 100
  # 
  # Returns a data frame of simulated batch digests
  # with batch sizes in vector batch_sizes, default (1, 5, 10, 20, 50) individuals/batch
  # values reported as inferred CFU/individual and log10(CFU/individual)
  
  # get size of data
  capp<-dim(mydata)[1]
  
  # make someplace to put stuff
  temp<-vector("list", length=length(batch_sizes)*n_reps)
  
  for(i in seq_len(n_reps)){
    batch_data<-rep(0, length(batch_sizes))
    for (j in seq_along(batch_sizes)){
      # Randomly pull samples from data for batching
      idx<-sample(1:capp,batch_sizes[j],replace=TRUE)
      # Assume Poisson count error and generate new counts
      temp_count<-rpois(batch_sizes[j], mydata$Count[idx])
      # Calculate CFU/worm
      batch_data[j]<-mean(correction_constant*temp_count*FoldD^mydata$D[idx], na.rm=TRUE)
    }
    temp[[i]]<-tibble(Batch=batch_sizes,
                      FinalCount=batch_data) 
  }
  
  # unfold data
  dataSet<-dplyr::bind_rows(temp) # unpack
  
  dataSet<-dataSet %>%
    mutate(logCFU=log10(FinalCount))
  if (sum(is.infinite(dataSet$logCFU))>0){dataSet$logCFU[is.infinite(dataSet$logCFU)]<-0}
  
  return(dataSet)
}
```

The next function does the heavy lifting on the bootstrap. Note that the function will accept batch-based data, but the stability of this code for non-uniformly weighted batches has not been tested.
```{r}
bootOnCountsStats<-function(input_data, batch_sizes=c(1,5,10,20,50), nboot=1000, 
                            FoldD=10, correction_constant=20){
  # Function that performs many iterations of bootstrapping on counts
  # Assumes all data sets (indicated by unique values of Run) are independent replicates of one experiment
  # If this is true, results will indicate run-to-run variation and estimated false-positive rates
  # Calls bootOnCounts() for basic functionality.
  # 
  # IF A DATA SET OF INDIVIDUAL-BASED DATA IS PROVIDED
  #   - The data set must contain at least two runs of data, ideally with 18+ data points in each
  #   - Function assumes no subsampling (each individual is essentially a unit-1 subsample)
  #   - Generates resampled data plus Poisson noise for individual and batch-based measurements
  #   - Returns data statistics and t-test and Wilcoxon results of nboot resamples for each pair of samples at each batch size
  #
  # IF A DATA SET OF BATCH-BASED DATA IS PROVIDED
  #   - The data set must contain at least two runs of data, ideally with 18+ data points in each
  #   - Generates resampled data plus Poisson noise using the indicated weighting from input data
  #   - Returns data statistics and t-test and Wilcoxon results of nboot resamples for each pair of samples at each batch size
  # 
  # Takes a data set with columns
  #   Run (num): Each unique index represents one data set
  #   Count (num): Number of raw counts associated with each sample
  #   Batch (int): batch size or weight of each sample.
  #         Batch=1 is the minimum and indicates individual-based sampling.
  #   D (num, optional): Fold dilution at which counts were taken
  #         (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc)
  #         ***IF D IS NOT GIVEN, FinalCount MUST ALREADY EXIST IN THE DATA SET***
  #   FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling.
  #         Will be calculated if not present.
  # Each row of the data set represents one measurement.
  # 
  # Other inputs:
  #   batch_sizes (int): If suppling individual measurements, a vector of batch sizes is needed for
  #   FoldD (num): Fold dilution in dilution series. Default is 10X dilutions.
  #   correction_constant (num): a multiplier to correct the fraction of a sample used for one measurement to the original volume
  #        (e.g. counts from 10 uL spots and an initial volume of 1 mL require correction (1000/10)=100)
  
  # load the necessary
  pacman::p_load(tidyverse)
  
  ## START
  # Figure out what is in the data set
  # Do we need to correct counts for dilution?
  # Divide by batch size (may be 1) to get per-unit or per-individual numbers
  my_names<-names(input_data) 
  if(length(which(my_names=="FinalCount"))==0){  # if FinalCount does not exist
    if(length(which(my_names=="D"))!=0){ # if we are given a dilution factor
      input_data$FinalCount<-(correction_constant *(input_data$Count*FoldD^input_data$D))/Batch
    } else if(length(which(my_names=="D"))==0){
      stop("Either fold dilution D or FinalCount must be a column in input_data!")
    }
      else{
      input_data$FinalCount<-(correction_constant * input_data$Count)/Batch
    }
  } 
  # now we should have final counts
  
  # Are data individual or batch-based?
  isIndividual<-max(input_data$Batch == 1)
  
  # How many runs of data are in the input data set?
  run_ids<-unique(input_data$Run)
  n_runs<-length(run_ids)
  if(n_runs<2){
    stop("At least two runs of data are required for bootstrapping.")
  }
  
  # initiate looping index
  idx<-1
  
  if (isIndividual){  ## For individual-based data
    # Someplace to put the data generated
    temp<-vector("list", length=length(batch_sizes)*n_runs*n_runs*nboot)
    
    for (m in seq_len(nboot)){  
      for (i in seq_len(n_runs)){
        for (j in seq_len(n_runs)){ # For each pair of runs
            # Obtain two sets of data from the input
            temp1<-input_data %>%
              dplyr::filter(Run==run_ids[i])
            temp2<-input_data %>%
              dplyr::filter(Run==run_ids[j])
            
            # Bootstrap both sets
            Boot1<-bootOnCounts(n_reps=dim(temp1)[1], mydata=temp1, 
                                batch_sizes=batch_sizes, FoldD=FoldD, correction_constant = correction_constant)
            Boot2<-bootOnCounts(n_reps=dim(temp2)[1], mydata=temp2, 
                                batch_sizes=batch_sizes, FoldD=FoldD, correction_constant = correction_constant)
            
            # fix any zeros thrown by resample
            if (sum(is.infinite(Boot1$logCFU))>0){Boot1$logCFU[is.infinite(Boot1$logCFU)]<-0}
            if (sum(is.infinite(Boot2$logCFU))>0){Boot2$logCFU[is.infinite(Boot2$logCFU)]<-0}
    
            # generate stats and carry out tests
            for (k in seq_along(batch_sizes)){
              Boot1s<-Boot1 %>%
                filter(Batch==batch_sizes[k])
              Boot2s<-Boot2 %>%
                filter(Batch==batch_sizes[k])
              Boot.t<-t.test(Boot1s$logCFU, Boot2s$logCFU)
              Boot.w<-wilcox.test(Boot1s$logCFU, Boot2s$logCFU)
              sw1<-shapiro.test(Boot1s$logCFU)
              sw2<-shapiro.test(Boot2s$logCFU)
              
              # store the results
              temp[[idx]]<-tibble(
                Boot=m,
                Batch=batch_sizes[k],
                Run1=i,
                Run2=j,
                meanCFU1=mean(Boot1s$FinalCount, na.rm=TRUE), 
                meanCFU2=mean(Boot2s$FinalCount, na.rm=TRUE),
                varCFU1=var(Boot1s$FinalCount, na.rm=TRUE), 
                varCFU2=var(Boot2s$FinalCount, na.rm=TRUE),
                cvCFU1=sd(Boot1s$FinalCount, na.rm=TRUE)/mean(Boot1s$FinalCount, na.rm=TRUE),
                cvCFU2=sd(Boot2s$FinalCount, na.rm=TRUE)/mean(Boot2s$FinalCount, na.rm=TRUE),
                p_sw1=sw1$p.value, 
                p_sw2=sw2$p.value,
                p_t=Boot.t$p.value,
                p_w=Boot.w$p.value
              )
              idx<-idx+1
              } # finish loop over batch sizes
          }
        } # finish loops over run pairs
    } # finish loop over bootstraps
  } else {# finish individual conditional
    # Someplace to put the data generated
    temp<-vector("list", length=n_runs*n_runs*nboot)
    
    # If we have batched data
    # Can't extrapolate to other batch sizes, but we can generate summary stats for the batching in data
    for (m in seq_len(nboot)){  
      for (i in seq_len(n_runs)){
        for (j in seq_len(n_runs)){ # For each pair of runs
          # Obtain two sets of data from the input
          temp1<-input_data %>%
            dplyr::filter(Run==run_ids[i])
          temp2<-input_data %>%
            dplyr::filter(Run==run_ids[j])
          n1<-dim(temp1)[1] # get sizes
          n2<-dim(temp2)[1]
          # Resamples with batch weights from data
          idx1<-sample(1:n1,n1,replace=TRUE)
          idx2<-sample(1:n2,n2,replace=TRUE)
          # Assume Poisson count error and generate new counts
          temp_count_1<-rpois(n1, temp1$Count[idx1])
          temp_count_2<-rpois(n2, temp2$Count[idx2])
          # Calculate biologically averaged total counts for resampled data
          Boot1s<-(correction_constant*temp_count_1*FoldD^temp1$D[idx1])/temp1$Batch
          Boot2s<-(correction_constant*temp_count_2*FoldD^temp2$D[idx2])/temp2$Batch
          
          Boot.t<-t.test(log10(Boot1s), log10(Boot2s))
          Boot.w<-wilcox.test(Boot1s, Boot2s)
          sw1<-shapiro.test(log10(Boot1s))
          sw2<-shapiro.test(log10(Boot2s))
          
          # sort data for quantile calculations
          Boot1s<-sort(Boot1s)
          Boot2s<-sort(Boot2s)
          
          # get values for Hogg's calculations#
            L05_1=mean(Boot1s[1:ceiling(n1/20)])
            U05_1=mean(Boot1s[-(1:(n1-ceiling(n1/20)))], na.rm=TRUE)
            L5_1=mean(Boot1s[1:round(n1/2)], na.rm=TRUE)
            U5_1=mean(Boot1s[-(1:round(n1/2))], na.rm=TRUE)
            M5_1=mean(Boot1s[ceiling(n1/4):floor(n1*0.75)], na.rm=TRUE)
            L05_2=mean(Boot2s[1:ceiling(n2/20)])
            U05_2=mean(Boot2s[-(1:(n2-ceiling(n2/20)))], na.rm=TRUE)
            L5_2=mean(Boot2s[1:round(n2/2)], na.rm=TRUE)
            U5_2=mean(Boot2s[-(1:round(n2/2))], na.rm=TRUE)
            M5_2=mean(Boot2s[ceiling(n2/4):floor(n2*0.75)], na.rm=TRUE)
            
          # store the results
          temp[[idx]]<-tibble(
            Boot=m,
            Run1=i,
            Run2=j,
            meanCFU1=mean(Boot1s, na.rm=TRUE), 
            meanCFU2=mean(Boot2s, na.rm=TRUE),
            varCFU1=var(Boot1s, na.rm=TRUE), 
            varCFU2=var(Boot2s, na.rm=TRUE),
            cvCFU1=sd(Boot1s, na.rm=TRUE)/mean(Boot1s, na.rm=TRUE),
            cvCFU2=sd(Boot2s, na.rm=TRUE)/mean(Boot2s, na.rm=TRUE),
            Q1_1=(U05_1-M5_1)/(M5_1-L05_1), # Hogg's stats
            Q2_1=(U05_1-L05_1)/(U5_1-L5_1),
            Q1_2=(U05_2-M5_2)/(M5_2-L05_2),
            Q2_2=(U05_2-L05_2)/(U5_2-L5_2),
            # store p-values
            p_sw1=sw1$p.value, 
            p_sw2=sw2$p.value,
            p_t=Boot.t$p.value,
            p_w=Boot.w$p.value
          )
          idx<-idx+1
        }
      } # finish loop over run pairs
    } # finish loop over bootstraps
  } # end else loop
  
  # unfold and return results
  dataSet<-dplyr::bind_rows(temp)
  return(dataSet)
}
```

This is a short function for returning summary statistics from a data set of counts, including the less commonly used Hogg's Q1 and Q2:
```{r}
summaryStats<-function(input_data, groups_in=c("Condition", "Run", "Batch"), FoldD=10, correction_constant=20){
    # Takes a data set with columns
  #   Run (num): Each unique index represents one data set
  #   Count (num): Number of raw counts associated with each sample
  #   Batch (int): batch size or weight of each sample.
  #         Batch=1 is the minimum and indicates individual-based sampling.
  #   D (num, optional): Fold dilution at which counts were taken
  #         (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc)
  #         ***IF D IS NOT GIVEN, FinalCount MUST ALREADY EXIST IN THE DATA SET***
  #   FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling.
  #         Will be calculated if not present.
  # Each row of the data set represents one measurement.
  # 
  # Other inputs:
  #   groups_in (char): Vector of column names to be used in group_by() for summaries
  #   FoldD (num): Fold dilution in dilution series. Default is 10X dilutions.
  #   correction_constant (num): a multiplier to correct the fraction of a sample used for one measurement to the original volume
  #        (e.g. counts from 10 uL spots and an initial volume of 1 mL require correction (1000/10)=100)
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  # First - figure out what is in the data set
  # Do we need to correct counts for dilution?
  # Divide by batch size (may be 1) to get per-unit or per-individual numbers
  my_names<-names(input_data) 
  if(length(which(my_names=="FinalCount"))==0){  # if FinalCount does not exist
    if(length(which(my_names=="D"))!=0){ # if we are given a dilution factor
      input_data$FinalCount<-(correction_constant *(input_data$Count*FoldD^input_data$D))/Batch
    } else if(length(which(my_names=="D"))==0){
      stop("Either fold dilution D or FinalCount must be a column in input_data!")
    }
      else{
      input_data$FinalCount<-(correction_constant * input_data$Count)/Batch
    }
  } 
  # now we should have final counts
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # 
  
  data_summary<-input_data %>%
    sort_by(.$FinalCount) %>% # arrange data in increasing order by total count
    group_by(pick(groups_in)) %>% # depreciated idc
    summarize(n=n(),
              meanC=mean(FinalCount, na.rm=TRUE),
              medianC=median(FinalCount, na.rm=TRUE),
              varC=var(FinalCount, na.rm=TRUE),
              L05=mean(FinalCount[1:ceiling(n()/20)], na.rm=TRUE), # get values for Hogg's calculations#
              U05=mean(FinalCount[-(1:(n()-ceiling(n()/20)))], na.rm=TRUE),
              L5=mean(FinalCount[1:round(n()/2)], na.rm=TRUE),
              U5=mean(FinalCount[-(1:round(n()/2))], na.rm=TRUE),
              M5=mean(FinalCount[ceiling(n()/4):floor(n()*0.75)], na.rm=TRUE)
              ) %>%
    mutate(
      Q1=(U05-M5)/(M5-L05),
	  	Q2=(U05-L05)/(U5-L5)
	  	)
  if (min(data_summary$n<20)){print("Warning: Hogg's statistics require n>20")}
  return(data_summary)
}
```



# If You Have Individual-Based Data
To assess run to run variation and false positive rates from individual-based data, you will need data from at least two independent runs (not internal/technical replicates) of the same experiment. Ideally each run will contain a sufficient number of data points to allow reasonable bootstrapping; for worms, we strongly recommend a minimum sample size of 18 individuals.

If individual-based data are available, it is possible to use bootstrapping to empirically assess the probable effects of biological averaging. The workflow here will recapitulate the results in Figure 1 of the manuscript; users should replace our data (bacterial load for Salmonella enterica in N2 worms) with their own data set.

First, load in the data. In this example, we use data from the manuscript; the full data file also contains data for colonization by Staphylococcus aureus that we will filter out.


```{r}
SaSeCountAll<-read_xlsx("SaSeCount.xlsx")  # full data file including single worm data

input_data<-SaSeCountAll %>%
  dplyr::filter(Condition=="SE" & Batch==1)
```

The bootstrapping functions expect count-based data with a particular structure. Each row of the data set represents one measurement, and the data should have columns: 
  * Run (num): Each unique index represents one data set
  * Count (num): Number of raw counts associated with each sample
  * Batch (int): batch size or weight of each sample. Batch=1 is the minimum and indicates individual-based sampling.
  * D (num, optional): Fold dilution at which counts were taken (e.g. D=0 indicates undiluted sample; D=1 indicates the first FoldD-fold dilution; etc) ***IF D IS NOT GIVEN, FinalCount MUST ALREADY EXIST IN THE DATA SET***
  * FinalCount (num, optional): Total inferred counts, based on raw counts and adjusted for dilution and sampling. (This will be calculated if not present, if D is provided. Otherwise an error is returned.)
  
Let's make sure the data structure is OK before proceeding.
```{r}
glimpse(input_data) # take a quick look at the data object contents and confirm column names are OK
```


The resulting data set contains three independent runs of individual-based data, all of which are replicates of the same experimental condition (colonization of N2 adults by S. enterica). A quick plot is a useful check at this point:
```{r}
input_data %>%
  ggplot(aes(x=factor(Run), y=logCFU, color=factor(Run))) + 
  geom_jitter(shape=16, position=position_jitter(0.05)) +
  geom_violin(fill=NA) + 
  ylim(-0.1,6)+ theme_classic() + 
  theme(
    text=element_text(size=14), 
    axis.title.x = element_blank(), 
    plot.title=element_text(hjust=0.5,size=14),
    legend.position = "none") +
  facet_wrap(vars(Condition), scales="free_x")
```
We can also generate summary statistics for the data:
```{r}
summaryStats(input_data = input_data)
```


If the data look as expected, we can proceed to bootstrap. It may be smart to do one run of a bootstrap as a sanity check, to get a feel for what the results will look like. 
```{r}
# Pull out two runs from the data set
temp1<-input_data %>%
  dplyr::filter(Condition=="SE" & Run=="3")
temp2<-input_data %>%
  dplyr::filter(Condition=="SE" & Run=="2")

# For each run, generate bootstrapped data
# Here we are allowing defaults including batch_sizes=c(1, 5, 10, 20, 50)
Boot1<-bootOnCounts(n_reps=dim(temp1)[1], mydata=temp1)
Boot2<-bootOnCounts(n_reps=dim(temp2)[1], mydata=temp2)

# Name the runs something useful
Boot1$Run<-as.factor("Run3")
Boot2$Run<-as.factor("Run2")

# and combine into a single object for easy plotting
jointBoot<-rbind(Boot1, Boot2)

# easy plotting
jointBoot %>%
	ggplot(aes(x=factor(Run), y=logCFU, color=factor(Run))) + 
	geom_jitter(shape=16, position=position_jitter(0.05)) +
	geom_violin(fill=NA) + 
  geom_hline(yintercept=log10(20), color="black", lty="dashed")+
	ylim(-0.1,6.2) + 
  theme_classic() +
  scale_color_viridis_d(begin=0.3, end=0.8) +
	theme(
	  text=element_text(size=xTextSize), 
	  axis.title.x = element_blank(),
	  axis.text.x = element_blank(),
	  axis.text.y = element_text(size=xTextSize-1),
	  axis.title.y = element_text(size=xTextSize),
	  plot.title=element_text(hjust=0.5,size=xTextSize),
	  #legend.position.inside=c(0.9,0.3),
	  legend.title = element_blank(),
	  legend.text = element_text(size=xTextSize-1)
	  )+
  facet_wrap(vars(Batch), ncol=5)+
  labs(title=expression(paste(italic("Bacteria"), " Simulated Batch Digests")), 
       y=expression(log[10](CFU)), x="Run")+
  stat_compare_means(method="t.test", label.y = 6.2, size=3.2)+
  stat_compare_means(method="wilcox.test", label.y=5.9, size=3.2)
```

If you are satisfied that your data look ok, and that the bootstrap is proceeding correctly, you can proceed to generate a large number of bootstrap data sets and calculate the associated statistics of interest. The function *bootonCountsStats()* will do this, returning summary statistics for bootstrapped data and between-group comparisons:
```{r}
# Carries out nboot simulations and stores
BootCombinations<-bootOnCountsStats(input_data=input_data, batch_sizes=c(1,5,10,20,50), nboot=1000, 
                                      FoldD=10, correction_constant=20)
# If you want more human-interpretable run IDs, they must be added after the bootstrap
BootCombinations$Run1ID<-paste("Run", BootCombinations$Run1, sep=" ")
BootCombinations$Run2ID<-paste("Run", BootCombinations$Run2, sep=" ")

# look at contents
glimpse(BootCombinations)
```


Get and plot the fraction of runs in which the pairwise test indicates a significant difference between groups. Comparisons of each data set to itself should always have false positive rates close to the declared value alpha.
```{r}
# how many comparisons are significant for each test?
BootCombinations<-BootCombinations %>%
  mutate(isSig_t=as.integer(as.logical(p_t<0.05)), # label significant t-tests
         isSig_w=as.integer(as.logical(p_w<0.05)), # and significant Wilcoxon tests
         RunPair=paste(Run1ID, "v", Run2ID, sep=" ")) # and label each result with the pairs of runs compared

# Summarize to get fraction of bootstrap replicates where each test is significant
# for each pair of runs at each batch size
BootCombinations_testsummary<-BootCombinations %>%
  group_by(RunPair, Batch) %>%
  summarize(countSigT=sum(isSig_t),
            countSigW=sum(isSig_w),
            n=n()) %>%
  mutate(fracSigT=countSigT/n,
         fracSigW=countSigW/n) %>%
  ungroup() %>%
  dplyr::select(RunPair, Batch, fracSigT, fracSigW) %>%
  pivot_longer(cols=starts_with("fracSig"), names_to = "test", 
               names_prefix = "fracSig", values_to = "fracSig")

# Remove redundant pairs ('Run 1 v Run 3' is the same as 'Run3 v Run 1')
# This list works if you have three runs -
# change your list accordingly
pair_list=c("Run 1 v Run 1", "Run 2 v Run 2", "Run 3 v Run 3",
            "Run 1 v Run 2","Run 1 v Run 3","Run 2 v Run 3")
# plot
BootCombinations_testsummary %>%
  dplyr::filter(RunPair %in% pair_list) %>%
  ggplot(aes(x=factor(Batch), y=fracSig, fill=factor(test)))+
  geom_col(position=position_dodge())+
  geom_hline(yintercept=0.05)+
  scale_fill_manual(labels = c("t-test", "Wilcoxon"), values = c("blue", "red")) +
  theme_bw()+
  theme(
    text=element_text(size=xTextSize), 
    plot.title=element_text(hjust=0.5)
  )+
  labs(x="Batch Size", y="Fraction Significant", fill="Test",
       title="Pairwise Tests")+
  facet_wrap(~RunPair)
```

Having estimated the effects of batching on error rates in pairwise comparisons, we next want to estimate the magnitude of run-to-run variation. This will determine whether thresholding comparisons based on a minimum empirical effect size will correct false-positive inflation due to batching. 

We can do this empirically using the measures of run-to-run distance from Figure 2:
```{r}


```



# If You Do Not Have Individual-Based Data
